{"cells":[{"cell_type":"markdown","source":["# Graph processing with Azure Databricks and Cosmos DB\n\nIn this notebook, you will download some bike trips data (stored in parquet files on Azure Data Lake Storage Gen2) and then use the Spark cluster run by Azure Databricks to persist the data as a graph in Azure Cosmos DB.\n\nThis sample application is meant to provide you with template code to process and persist a graph in Azure Cosmos DB. You will then be able to test the Cosmos DB graph processing capabilities using the Cosmos DB Gremlin API. The Gremlin API supports modeling graph data and provides APIs to traverse through the graph data."],"metadata":{}},{"cell_type":"markdown","source":["## Attach notebook to you cluster\n\nBefore executing any cells in the notebook, you need to attach it to your cluster. In the notebook's toolbar, select the drop down arrow next to Detached, and then select your cluster under Attach to.\n\n![Detach is expanded in the notebook toolbar, and the cluster is highlighted under Attach to.](https://github.com/alagala/labs/raw/master/azure/databricks/media/databricks-attach-notebook.png \"Attach notebook\")"],"metadata":{}},{"cell_type":"markdown","source":["## Download the sample parquet files (with bike trips data) on the cluster local store"],"metadata":{}},{"cell_type":"code","source":["%sh\nwget -P /tmp https://raw.githubusercontent.com/alagala/labs/master/azure/cosmosdb/graph/data/stations.snappy.parquet\nwget -P /tmp https://raw.githubusercontent.com/alagala/labs/master/azure/cosmosdb/graph/data/trips.snappy.parquet"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## Connect to the Azure Data Lake Storage Gen2 (ADLS)\n\nThe snippet of code below connects to ADLS and uploads the parquet. Then, it lists all files available on the ADLS filesystem.\n\nIn production, you would probably use services like [Azure Data Factory](https://docs.microsoft.com/en-us/azure/data-factory/) to copy data from external data sources to the Azure Data Lake Storage.\n\n> **NOTE**: For this hands-on lab, the endpoint and master key have already been added to the Azure Key Vault service, so you will retrieve the values from there using `dbutils.secrets.get()`."],"metadata":{}},{"cell_type":"code","source":["# Variable declarations. These will be accessible by any calling notebook.\nkeyVaultScope = \"key-vault-secrets\"\nadlsAccountName = dbutils.secrets.get(keyVaultScope, \"ADLS-Gen2-Account-Name\")\nadlsAccountKey = dbutils.secrets.get(keyVaultScope, \"ADLS-Gen2-Account-Key\")\n\nabfsUri = \"abfss://biketrips@\" + adlsAccountName + \".dfs.core.windows.net/\"\n\nspark.conf.set(\"fs.azure.account.key.\" + adlsAccountName + \".dfs.core.windows.net\", adlsAccountKey)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Since the hierarchical namespace is enabled for the storage account, we must initialize a filesystem before we can access it.\nspark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\ndbutils.fs.ls(abfsUri)\nspark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")\n\ndbutils.fs.cp(\"file:///tmp/stations.snappy.parquet\", abfsUri)\ndbutils.fs.cp(\"file:///tmp/trips.snappy.parquet\", abfsUri)\n\ndbutils.fs.ls(abfsUri)"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Load the bike trip data"],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", \"16\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.sql.functions import col\n\nbikeStations = spark.read.parquet(abfsUri + \"stations.snappy.parquet\")\ntripData = spark.read.parquet(abfsUri + \"trips.snappy.parquet\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["bikeStations.show(5)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["tripData.show(5)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["## Build the graph\n\nThe first step is to build the graph. Therefore, we need to define the vertices (bike stations) and the edges (bike trips, from one station to the other). In our case we are building a _directed graph_: this graph will point from the source to the location. In the context of this bike trip data, this will point from a trip's starting location to a trip's ending location.\n\nTo define the graph, we use the naming conventions fro columns presented in the GraphFrames library. In the vertices table, we define our identifer as `id`, and in the edges table we label each edge's source vertex ID as `src` and the destination ID as `dst`."],"metadata":{}},{"cell_type":"code","source":["from graphframes import GraphFrame\n\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql.types import StringType\nfrom urllib.parse import quote\n\ndef urlencode(value):\n  return quote(value, safe=\"\")\n\nudf_urlencode = udf(urlencode, StringType())\n\nstationVertices = bikeStations \\\n  .withColumn(\"id\", col(\"station_id\").cast(StringType())) \\\n  .withColumn(\"station_id\", udf_urlencode(\"name\")) \\\n  .withColumn(\"type\", lit(\"station\")) \\\n  .distinct()\n\ntripEdges = tripData \\\n  .withColumn(\"id\", col(\"trip_id\").cast(StringType())) \\\n  .withColumn(\"src\", col(\"start_terminal\").cast(StringType())) \\\n  .withColumn(\"dst\", col(\"end_terminal\").cast(StringType())) \\\n  .withColumn(\"relationship\", lit(\"bike_to\")) \\\n  .drop(\"trip_id\", \"start_terminal\", \"end_terminal\")\n\nstationGraph = GraphFrame(stationVertices, tripEdges)\nstationGraph.cache()\n\n# Display some basic statistics about the graph.\nprint(\"Total number of stations: \" + str(stationGraph.vertices.count()))\nprint(\"Total number of trips in graph: \" + str(stationGraph.edges.count()))\nprint(\"Total number of trips in original data: \" + str(tripData.count()))"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["## Cosmos DB Graph backend format\n\nA Cosmos DB graph collection stores data in JSON format in the backend. A detailed description of this representation of vertices and edges is described [here](https://github.com/LuisBosquez/azure-cosmos-db-graph-working-guides/blob/master/graph-backend-json.md)."],"metadata":{}},{"cell_type":"markdown","source":["### Preparing Vertices\n\nFrom the GraphFrame vertices, we’ll create a new DataFrame containing Cosmos DB vertex rows. Each such row contains the following columns:\n\n- `id`: Unique ID of the vertex . **Note**: `id` in Cosmos DB is part of the resource URI and hence, if it is a string, it must be URL encoded;\n- `label` (_optional_): The type of vertex entity (in our example we could say the type is `station`);\n- Partition key: If the Cosmos DB graph is provisioned as a partitioned collection, there must be a column with that partition key name (in our example, we have indicated the partition key to be `station_id`);\n- Property bag: Gremlin vertex properties are stored as Arrays of Struct, because multiple values are allowed in a single property (**Note**: partition key property is stored as a regular column and NOT a property bag)."],"metadata":{}},{"cell_type":"code","source":["def to_cosmosdb_vertices(dfVertices, labelColumn, partitionKey = \"\"):\n  columns = [\"id\", labelColumn]\n  \n  if partitionKey:\n    columns.append(partitionKey)\n  \n  columns.extend(['nvl2({x}, array(named_struct(\"id\", uuid(), \"_value\", {x})), NULL) AS {x}'.format(x=x) \\\n                  for x in dfVertices.columns if x not in columns])\n \n  return dfVertices.selectExpr(*columns).withColumnRenamed(labelColumn, \"label\")\n\ncosmosDbVertices = to_cosmosdb_vertices(stationGraph.vertices, \"type\", \"station_id\")\ndisplay(cosmosDbVertices)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### Preparing Edges\n\nNow its time to transform GraphFrame edges into a Cosmos DB DataFrame. Each row inside that DataFrame will contain the following columns:\n\n- `id`: Similar to the `id` column in a vertex row, a unique ID of the edge;\n- `label`: The name of edge relationship;\n- Gremlin edge properties are stored as regular columns, since multi-valued properties are not supported in Gremlin edges;\n- `_isEdge`: Hardcoded boolean column with value `True`;\n- `_vertexId`: ID of the source vertex;\n- `_sink`: ID of the destination vertex;\n\nSimilar to vertices, if the Cosmos DB graph is provisioned as a partitioned collection, the following additional columns must also be provided:\n\n- Partition key: Column with the source vertex partition key name;\n- `_sinkPartition`: Value of partition key of destination vertex."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import concat_ws, col\n\ndef to_cosmosdb_edges(g, labelColumn, partitionKey = \"\"): \n  dfEdges = g.edges\n  \n  if partitionKey:\n    dfEdges = dfEdges.alias(\"e\") \\\n      .join(g.vertices.alias(\"sv\"), col(\"e.src\") == col(\"sv.id\")) \\\n      .join(g.vertices.alias(\"dv\"), col(\"e.dst\") == col(\"dv.id\")) \\\n      .selectExpr(\"e.*\", \"sv.\" + partitionKey, \"dv.\" + partitionKey + \" AS _sinkPartition\")\n\n  dfEdges = dfEdges \\\n    .withColumn(\"_isEdge\", lit(True)) \\\n    .withColumn(\"_vertexId\", col(\"src\")) \\\n    .withColumn(\"_sink\", col(\"dst\")) \\\n    .withColumnRenamed(labelColumn, \"label\") \\\n    .drop(\"src\", \"dst\")\n  \n  return dfEdges\n\ncosmosDbEdges = to_cosmosdb_edges(stationGraph, \"relationship\", \"station_id\")\ndisplay(cosmosDbEdges)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["## Store the graph in Cosmos DB\n\nUsing the Azure Cosmos DB Spark Connector, you will now persist the graph to Azure Cosmos DB where users and applications can connect for interactive queries.\n\nIn order to write to Cosmos DB, you need to first create a configuration object that contains the configuration information. If you are curious, read the [configuration reference](https://github.com/Azure/azure-cosmosdb-spark/wiki/Configuration-references) for details on all of the options. \n\nThe core items you need to provide are:\n\n  - **Endpoint**: Your Cosmos DB url (i.e. https://youraccount.documents.azure.com:443/).\n  - **Masterkey**: The primary or secondary key string for you Cosmos DB account.\n  - **Database**: The name of the database.\n  - **Collection**: The name of the collection that you wish to query.\n\n> **NOTE**: For this hands-on lab, the endpoint and master key have already been added to the Azure Key Vault service, so you will retrieve the values from there using `dbutils.secrets.get()`."],"metadata":{}},{"cell_type":"code","source":["cosmosDbConfig = {\n    \"Endpoint\" : dbutils.secrets.get(keyVaultScope, \"Cosmos-DB-URI\"),\n    \"Masterkey\" : dbutils.secrets.get(keyVaultScope, \"Cosmos-DB-Key\"),\n    \"Database\" : \"BikeTrips\",\n    \"Collection\" : \"trips\",\n    \"Upsert\" : \"true\",\n    \"WritingBatchSize\": \"2000\"\n}\n\ncosmosDbFormat = \"com.microsoft.azure.cosmosdb.spark\"\n\ncosmosDbVertices.write.format(cosmosDbFormat).mode(\"append\").options(**cosmosDbConfig).save()\ncosmosDbEdges.write.format(cosmosDbFormat).mode(\"append\").options(**cosmosDbConfig).save()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":22}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.8","nbconvert_exporter":"python","file_extension":".py"},"name":"Process-Cosmos-DB-Change-Feed","notebookId":730817167708027},"nbformat":4,"nbformat_minor":0}
