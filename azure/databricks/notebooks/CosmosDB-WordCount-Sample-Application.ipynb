{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access an Azure Data Lake Storage\n",
    "To access an Azure Data Lake Storage Gen2 storage account, we recommend that you set your account credentials in your notebookâ€™s session configs.\n",
    "Use the storage account key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sh\n",
    "wget -P /tmp https://raw.githubusercontent.com/alagala/labs/azure/databricks/data/master/sample.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable declarations. These will be accessible by any calling notebook.\n",
    "keyVaultScope = \"key-vault-secrets\"\n",
    "adlsAccountName = dbutils.secrets.get(keyVaultScope, \"ADLS-Gen2-Name\")\n",
    "adlsAccountKey = dbutils.secrets.get(keyVaultScope, \"ADLS-Gen2-Key\")\n",
    "\n",
    "fileSystemName = \"data\"\n",
    "abfsUri = \"abfss://\" + fileSystemName + \"@\" + adlsAccountName + \".dfs.core.windows.net/\"\n",
    "\n",
    "spark.conf.set(\n",
    "  \"fs.azure.account.key.\" + adlsAccountName + \".dfs.core.windows.net\",\n",
    "  dbutils.secrets.get(scope = keyVaultScope, key = adlsAccountKey))\n",
    "\n",
    "dbutils.fs.cp(\"file:///tmp/sample.txt\", abfsUri)\n",
    "\n",
    "dbutils.fs.ls(absUri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is set up, you can use standard Spark and Databricks APIs to read from the storage account. For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val df = spark.read.text(abfsUri + \"sample.txt\")\n",
    "val groupedDf = df.toLowerCase.split(\" \")).groupBy(\"value\").count()\n",
    "\n",
    "groupedDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Cosmos DB\n",
    "\n",
    "Using the Azure Cosmos DB Spark Connector, you will now use Cosmos DB as an input source to retrieve a sample of transaction data. You will start by setting up a static connection to Cosmos, and reading in a sample of the transaction data that is stored there.\n",
    "\n",
    "In order to query Cosmos DB, you need to first create a configuration object that contains the configuration information. If you are curious, read the [configuration reference](https://github.com/Azure/azure-cosmosdb-spark/wiki/Configuration-references) for details on all of the options. \n",
    "\n",
    "The core items you need to provide are:\n",
    "\n",
    "  - **Endpoint**: Your Cosmos DB url (i.e. https://youraccount.documents.azure.com:443/).\n",
    "  - **Masterkey**: The primary or secondary key string for you Cosmos DB account.\n",
    "  - **Database**: The name of the database.\n",
    "  - **Collection**: The name of the collection that you wish to query.\n",
    "\n",
    "> **NOTE**: For this hands-on lab, you have already added the endpoint and master key into Azure Key Vault, so you will retrieve the values from there using `dbutils.secrets.get()`. If you chose to use different database and collection names, you will need to update those in the cell below prior to running it.\n",
    "\n",
    "The `query_custom` property of the configuration is the query that will be executed against the transactions collection in Cosmos DB. For this example, you are only pulling in a small sample of the transaction data.\n",
    "\n",
    "Run the cell to add the configuration needed to create a static connection to Cosmos DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://github.com/Azure/azure-cosmosdb-spark/wiki/Configuration-references\n",
    "# https://docs.microsoft.com/en-us/azure/cosmos-db/spark-connector\n",
    "\n",
    "# Write configuration\n",
    "writeConfig = {\n",
    "    \"Endpoint\" : dbutils.secrets.get(keyVaultScope, \"Cosmos-DB-Cassandra-URI\"),\n",
    "    \"Masterkey\" : dbutils.secrets.get(keyVaultScope, \"Cosmos-DB-Cassandra-Key\"),\n",
    "    \"Database\" : \"DepartureDelays\",\n",
    "    \"Collection\" : \"flights_fromsea\",\n",
    "    \"Upsert\" : \"true\"\n",
    "}\n",
    "\n",
    "# Write to Cosmos DB from the flights DataFrame\n",
    "flights.write.format(\"com.microsoft.azure.cosmosdb.spark\").options(**writeConfig).save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist the transaction data to an Azure Databricks Delta table\n",
    "\n",
    "[Databricks Delta](https://docs.databricks.com/delta/delta-intro.html) delivers a powerful transactional storage layer by harnessing the power of Apache Spark and Databricks DBFS. The core abstraction of Delta is an optimized Spark table that:\n",
    "\n",
    "  - Stores data as Parquet files in DBFS.\n",
    "  - Maintains a transaction log that efficiently tracks changes to the table.\n",
    "\n",
    "You read and write data stored in the delta format using the same familiar Apache Spark SQL batch and streaming APIs that you use to work with Hive tables and DBFS directories. With the addition of the transaction log and other enhancements, Delta offers significant benefits:\n",
    "\n",
    "  - **ACID transactions**\n",
    "    - Multiple writers can simultaneously modify a dataset and see consistent views. For qualifications, see Multi-cluster writes.\n",
    "    - Writers can modify a dataset without interfering with jobs reading the dataset.\n",
    "  - **Fast read access**\n",
    "    - Automatic file management organizes data into large files that can be read efficiently.\n",
    "    - Statistics enable speeding up reads by 10-100x and and data skipping avoids reading irrelevant information.\n",
    "    \n",
    "To create your transactions Delta table, you will first write the cleaned dataset contained within the `static_transactions_clean` DataFrame to a folder in ADLS Gen2 in Databricks Delta format.\n",
    "\n",
    "Let's break down the command in the cell below before running it.\n",
    "\n",
    "  - `mode(\"overwrite\")`: This tells the write operation to overwrite any existing Delta table stored in the specified location.\n",
    "  - `format(\"delta\")`: To save the data in Delta format, you specify \"delta\" with the `format()` option of the `write` command.\n",
    "  - `partitionBy()`: When creating a new Delta table, you can optionally specify partition columns. Partitioning is used to speed up queries or DML that have predicates involving the partition columns. In this case, we are partitioning on the `ipCountryCode`, which is also the same field used to partition the data stored in Cosmos DB.\n",
    "  - `save()`: The `save` command accepts a location, which is where the underlying files for the Delta table will be stored. In our case, this is a location in the ADLS Gen2 filesystem, which you will provide using the abfs URI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_transactions_clean.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"ipCountryCode\").save(\"abfss://\" + fileSystemName + \"@\" + adlsGen2AccountName + \".dfs.core.windows.net/delta/transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have saved the clean transaction data into a ADLS Gen2 filesystem location in Delta format, you can create a Databricks global table which is backed by the Delta location you created above. Notice that the `LOCATION` specified in the `CREATE TABLE` query is the same as what you used to write the cleaned transaction data in Delta format above. Doing this allows the table in the Hive metastore to automatically inherit the schema, partitioning, and table properties of the existing data.\n",
    "\n",
    "**IMPORTANT**: You will need to add the name of your ADLS Gen2 account into the location value below, before running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE transactions\n",
    "USING DELTA\n",
    "LOCATION 'abfss://transactions@<your-adls-gen2-account-name>.dfs.core.windows.net/delta/transactions' -- TODO: Replace <your-adls-gen2-account-name> with your ADLS Gen2 account name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the `DESCRIBE DETAIL` SQL command to view information about schema, partitioning, table size, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE DETAIL transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you can use Spark SQL to query records in the Hive table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM transactions LIMIT 10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "name": "Process-Cosmos-DB-Change-Feed",
  "notebookId": 1043257095573127
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
