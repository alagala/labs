{"cells":[{"cell_type":"markdown","source":["# WordCount sample application\n\nIn this notebook, you will download a text file and then use the Spark cluster run by Azure Databricks to count each distinct word in the file.\n\nThis sample application is meant to provide you with template code to access the Azure Data Lake (blob) storage and Cosmos DB. The blob storage is accessed to read the text file and to write onto it the word count. Cosmos DB is also used to store the word count."],"metadata":{}},{"cell_type":"markdown","source":["## Attach notebook to you cluster\n\nBefore executing any cells in the notebook, you need to attach it to your cluster. In the notebook's toolbar, select the drop down arrow next to Detached, and then select your cluster under Attach to.\n\n![Detach is expanded in the notebook toolbar, and the cluster is highlighted under Attach to.](https://github.com/alagala/labs/raw/master/azure/databricks/media/databricks-attach-notebook.png \"Attach notebook\")"],"metadata":{}},{"cell_type":"markdown","source":["## Download the sample text file on the cluster local store"],"metadata":{}},{"cell_type":"code","source":["%sh\nwget -P /tmp https://raw.githubusercontent.com/alagala/labs/master/azure/databricks/notebooks/data/sample.txt"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## Connect to the Azure Data Lake storage\n\nThe snippet of code below connects to the Azure Data Lake (blob) storage and uploads the sample text file. Then, it lists all files available on the blob storage.\n\nFor more information, refer to the [Azure Databricks documentation](https://docs.databricks.com/spark/latest/data-sources/azure/azure-datalake-gen2.html).\n\n> **NOTE**: For this hands-on lab, the endpoint and master key have already been added to the Azure Key Vault service, so you will retrieve the values from there using `dbutils.secrets.get()`."],"metadata":{}},{"cell_type":"code","source":["# Variable declarations. These will be accessible by any calling notebook.\nkeyVaultScope = \"key-vault-secrets\"\nadlsAccountName = dbutils.secrets.get(keyVaultScope, \"ADLS-Gen2-Account-Name\")\nadlsAccountKey = dbutils.secrets.get(keyVaultScope, \"ADLS-Gen2-Account-Key\")\n\nabfsUri = \"abfss://wordcount@\" + adlsAccountName + \".dfs.core.windows.net/\"\n\n# Since the hierarchical namespace is enabled for the storage account, we must initialize a filesystem before we can access it.\nspark.conf.set(\"fs.azure.account.key.\" + adlsAccountName + \".dfs.core.windows.net\", adlsAccountKey)\nspark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\ndbutils.fs.ls(abfsUri)\nspark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")\n\ndbutils.fs.cp(\"file:///tmp/sample.txt\", abfsUri)\n\ndbutils.fs.ls(abfsUri)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## Run the word count program"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_replace\nfrom pyspark.sql.functions import lower\nfrom pyspark.sql.functions import trim\nfrom pyspark.sql.functions import split\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import desc\n\ndf = spark.read.text(abfsUri + \"sample.txt\")\n\nwordcount = df.select(\n  # Remove punctuation and explode each line of text into a row per word.\n  explode(split(trim(lower(regexp_replace(df.value, \"[,.!?:;]\", \"\"))), \" \")).alias(\"word\")\n).groupBy(\"word\").count().orderBy(desc(\"count\"))\n\nwordcount.show(10)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Connect to Cosmos DB\n\nUsing the Azure Cosmos DB Spark Connector, you will now use Cosmos DB as an input source to retrieve a sample of transaction data.\n\nIn order to write to Cosmos DB, you need to first create a configuration object that contains the configuration information. If you are curious, read the [configuration reference](https://github.com/Azure/azure-cosmosdb-spark/wiki/Configuration-references) for details on all of the options. \n\nThe core items you need to provide are:\n\n  - **Endpoint**: Your Cosmos DB url (i.e. https://youraccount.documents.azure.com:443/).\n  - **Masterkey**: The primary or secondary key string for you Cosmos DB account.\n  - **Database**: The name of the database.\n  - **Collection**: The name of the collection that you wish to query.\n\n> **NOTE**: For this hands-on lab, the endpoint and master key have already been added to the Azure Key Vault service, so you will retrieve the values from there using `dbutils.secrets.get()`. Remember to replace `<DATABASE_ID>` with the actual name of the database that you used when creating it."],"metadata":{}},{"cell_type":"code","source":["# https://github.com/Azure/azure-cosmosdb-spark/wiki/Configuration-references\n# https://docs.microsoft.com/en-us/azure/cosmos-db/spark-connector\n\n# Write configuration\nwriteConfig = {\n    \"Endpoint\" : dbutils.secrets.get(keyVaultScope, \"Cosmos-DB-URI\"),\n    \"Masterkey\" : dbutils.secrets.get(keyVaultScope, \"Cosmos-DB-Key\"),\n    \"Database\" : \"<DATABASE_ID>\", # TODO: replace <DATABASE_ID> with the actual name you used when creating the database in Cosmos DB\n    \"Collection\" : \"words\",\n    \"Upsert\" : \"true\"\n}\n\n# Write to Cosmos DB from the flights DataFrame\nwordcount.write.format(\"com.microsoft.azure.cosmosdb.spark\").options(**writeConfig).save()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Persist the transaction data to an Azure Databricks Delta table\n\n[Databricks Delta](https://docs.databricks.com/delta/delta-intro.html) delivers a powerful transactional storage layer by harnessing the power of Apache Spark and Databricks DBFS. The core abstraction of Delta is an optimized Spark table that:\n\n  - Stores data as Parquet files in DBFS.\n  - Maintains a transaction log that efficiently tracks changes to the table.\n\nYou read and write data stored in the delta format using the same familiar Apache Spark SQL batch and streaming APIs that you use to work with Hive tables and DBFS directories. With the addition of the transaction log and other enhancements, Delta offers significant benefits:\n\n  - **ACID transactions**\n    - Multiple writers can simultaneously modify a dataset and see consistent views. For qualifications, see Multi-cluster writes.\n    - Writers can modify a dataset without interfering with jobs reading the dataset.\n  - **Fast read access**\n    - Automatic file management organizes data into large files that can be read efficiently.\n    - Statistics enable speeding up reads by 10-100x and and data skipping avoids reading irrelevant information.\n    \nTo create your transactions Delta table, you will first write the word count DataFrame to a folder in Azure Data Lake (blob) storage in Databricks Delta format.\n\nLet's break down the command in the cell below before running it.\n\n  - `mode(\"overwrite\")`: This tells the write operation to overwrite any existing Delta table stored in the specified location.\n  - `format(\"delta\")`: To save the data in Delta format, you specify \"delta\" with the `format()` option of the `write` command.\n  - `partitionBy()`: When creating a new Delta table, you can **optionally** specify partition columns. Partitioning is used to speed up queries or DML that have predicates involving the partition columns.\n  - `save()`: The `save` command accepts a location, which is where the underlying files for the Delta table will be stored. In our case, this is a location in the Azure Data Lake storage, which you will provide using the abfs URI."],"metadata":{}},{"cell_type":"code","source":["wordcount.write.mode(\"overwrite\").format(\"delta\").save(abfsUri + \"delta\")"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["Now that you have saved the clean transaction data into a Azure Data Lake storage blob location in Delta format, you can create a Databricks global table which is backed by the Delta location you created above. Notice that the `LOCATION` specified in the `CREATE TABLE` query is the same as what you used to write the word count data in Delta format above. Doing this allows the table in the Hive metastore to automatically inherit the schema, partitioning, and table properties of the existing data.\n\n**IMPORTANT**: You will need to add the name of your Azure Data Lake storage account into the location value below, before running the cell."],"metadata":{}},{"cell_type":"code","source":["%sql\nCREATE TABLE wordcount\nUSING DELTA\n-- TODO: Replace <STORAGE_ACCOUNT_NAME> with the actual Azure Data Lake storage account name (eg. datainsightspoc27450)\nLOCATION 'abfss://wordcount@<STORAGE_ACCOUNT_NAME>.dfs.core.windows.net/delta'"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["You can use the `DESCRIBE DETAIL` SQL command to view information about schema, partitioning, table size, and so on."],"metadata":{}},{"cell_type":"code","source":["%sql\nDESCRIBE DETAIL wordcount"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["Finally, you can use Spark SQL to query records in the Hive table."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM wordcount\nORDER BY count DESC\nLIMIT 10"],"metadata":{},"outputs":[],"execution_count":18}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.8","nbconvert_exporter":"python","file_extension":".py"},"name":"Process-Cosmos-DB-Change-Feed","notebookId":2126772687681592},"nbformat":4,"nbformat_minor":0}
