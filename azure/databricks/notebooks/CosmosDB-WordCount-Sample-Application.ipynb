{"cells":[{"cell_type":"markdown","source":["## Access an Azure Data Lake Storage\nTo access an Azure Data Lake Storage Gen2 storage account, we recommend that you set your account credentials in your notebook’s session configs.\nUse the storage account key."],"metadata":{}},{"cell_type":"code","source":["%sh\nwget -P /tmp https://raw.githubusercontent.com/alagala/labs/master/azure/databricks/notebooks/data/sample.txt"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">--2019-04-08 18:19:38--  https://raw.githubusercontent.com/alagala/labs/master/azure/databricks/notebooks/data/sample.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 689 [text/plain]\nSaving to: ‘/tmp/sample.txt’\n\n     0K                                                       100%  146M=0s\n\n2019-04-08 18:19:38 (146 MB/s) - ‘/tmp/sample.txt’ saved [689/689]\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Variable declarations. These will be accessible by any calling notebook.\nkeyVaultScope = \"key-vault-secrets\"\nadlsAccountName = dbutils.secrets.get(keyVaultScope, \"ADLS-Gen2-Account-Name\")\nadlsAccountKey = dbutils.secrets.get(keyVaultScope, \"ADLS-Gen2-Account-Key\")\n\n# TODO: replace <CONTAINER_NAME> with the actual name you used when creating the container.\nfileSystemName = \"datainsights\"\nabfsUri = \"abfss://\" + fileSystemName + \"@\" + adlsAccountName + \".dfs.core.windows.net/\"\n\nspark.conf.set(\"fs.azure.account.key.\" + adlsAccountName + \".dfs.core.windows.net\", adlsAccountKey)\n\ndbutils.fs.cp(\"file:///tmp/sample.txt\", abfsUri)\n\ndbutils.fs.ls(abfsUri)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">5</span><span class=\"ansired\">]: </span>[FileInfo(path=&apos;abfss://datainsights@[REDACTED].dfs.core.windows.net/sample.txt&apos;, name=&apos;sample.txt&apos;, size=689)]\n</div>"]}}],"execution_count":3},{"cell_type":"markdown","source":["Once this is set up, you can use standard Spark and Databricks APIs to read from the storage account. For example,"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_replace\nfrom pyspark.sql.functions import lower\nfrom pyspark.sql.functions import trim\nfrom pyspark.sql.functions import split\nfrom pyspark.sql.functions import explode\nfrom pyspark.sql.functions import desc\n\ndf = spark.read.text(abfsUri + \"sample.txt\")\n\nwordcount = df.select(\n  # Remove punctuation and explode each line of text into a row per word.\n  explode(split(trim(lower(regexp_replace(df.value, \"[,.!?:;]\", \"\"))), \" \")).alias(\"word\")\n).groupBy(\"word\").count().orderBy(desc(\"count\"))\n\nwordcount.show(10)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+-----+\n     word|count|\n+---------+-----+\n      the|    9|\nlanguages|    5|\n    their|    4|\n   common|    4|\n      and|    4|\n       to|    3|\n       of|    3|\n     more|    3|\n       be|    3|\n language|    3|\n+---------+-----+\nonly showing top 10 rows\n\n</div>"]}}],"execution_count":5},{"cell_type":"markdown","source":["## Connect to Cosmos DB\n\nUsing the Azure Cosmos DB Spark Connector, you will now use Cosmos DB as an input source to retrieve a sample of transaction data. You will start by setting up a static connection to Cosmos, and reading in a sample of the transaction data that is stored there.\n\nIn order to query Cosmos DB, you need to first create a configuration object that contains the configuration information. If you are curious, read the [configuration reference](https://github.com/Azure/azure-cosmosdb-spark/wiki/Configuration-references) for details on all of the options. \n\nThe core items you need to provide are:\n\n  - **Endpoint**: Your Cosmos DB url (i.e. https://youraccount.documents.azure.com:443/).\n  - **Masterkey**: The primary or secondary key string for you Cosmos DB account.\n  - **Database**: The name of the database.\n  - **Collection**: The name of the collection that you wish to query.\n\n> **NOTE**: For this hands-on lab, you have already added the endpoint and master key into Azure Key Vault, so you will retrieve the values from there using `dbutils.secrets.get()`. If you chose to use different database and collection names, you will need to update those in the cell below prior to running it.\n\nThe `query_custom` property of the configuration is the query that will be executed against the transactions collection in Cosmos DB. For this example, you are only pulling in a small sample of the transaction data.\n\nRun the cell to add the configuration needed to create a static connection to Cosmos DB."],"metadata":{}},{"cell_type":"code","source":["# https://github.com/Azure/azure-cosmosdb-spark/wiki/Configuration-references\n# https://docs.microsoft.com/en-us/azure/cosmos-db/spark-connector\n\n# Write configuration\nwriteConfig = {\n    \"Endpoint\" : dbutils.secrets.get(keyVaultScope, \"Cosmos-DB-URI\"),\n    \"Masterkey\" : dbutils.secrets.get(keyVaultScope, \"Cosmos-DB-Key\"),\n    \"Database\" : \"WordCount\",\n    \"Collection\" : \"words\",\n    \"Upsert\" : \"true\"\n}\n\n# Write to Cosmos DB from the flights DataFrame\nwordcount.write.format(\"com.microsoft.azure.cosmosdb.spark\").options(**writeConfig).save()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["## Persist the transaction data to an Azure Databricks Delta table\n\n[Databricks Delta](https://docs.databricks.com/delta/delta-intro.html) delivers a powerful transactional storage layer by harnessing the power of Apache Spark and Databricks DBFS. The core abstraction of Delta is an optimized Spark table that:\n\n  - Stores data as Parquet files in DBFS.\n  - Maintains a transaction log that efficiently tracks changes to the table.\n\nYou read and write data stored in the delta format using the same familiar Apache Spark SQL batch and streaming APIs that you use to work with Hive tables and DBFS directories. With the addition of the transaction log and other enhancements, Delta offers significant benefits:\n\n  - **ACID transactions**\n    - Multiple writers can simultaneously modify a dataset and see consistent views. For qualifications, see Multi-cluster writes.\n    - Writers can modify a dataset without interfering with jobs reading the dataset.\n  - **Fast read access**\n    - Automatic file management organizes data into large files that can be read efficiently.\n    - Statistics enable speeding up reads by 10-100x and and data skipping avoids reading irrelevant information.\n    \nTo create your transactions Delta table, you will first write the cleaned dataset contained within the `static_transactions_clean` DataFrame to a folder in ADLS Gen2 in Databricks Delta format.\n\nLet's break down the command in the cell below before running it.\n\n  - `mode(\"overwrite\")`: This tells the write operation to overwrite any existing Delta table stored in the specified location.\n  - `format(\"delta\")`: To save the data in Delta format, you specify \"delta\" with the `format()` option of the `write` command.\n  - `partitionBy()`: When creating a new Delta table, you can optionally specify partition columns. Partitioning is used to speed up queries or DML that have predicates involving the partition columns. In this case, we are partitioning on the `ipCountryCode`, which is also the same field used to partition the data stored in Cosmos DB.\n  - `save()`: The `save` command accepts a location, which is where the underlying files for the Delta table will be stored. In our case, this is a location in the ADLS Gen2 filesystem, which you will provide using the abfs URI."],"metadata":{}},{"cell_type":"code","source":["static_transactions_clean.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"ipCountryCode\").save(\"abfss://\" + fileSystemName + \"@\" + adlsGen2AccountName + \".dfs.core.windows.net/delta/transactions\")"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Now that you have saved the clean transaction data into a ADLS Gen2 filesystem location in Delta format, you can create a Databricks global table which is backed by the Delta location you created above. Notice that the `LOCATION` specified in the `CREATE TABLE` query is the same as what you used to write the cleaned transaction data in Delta format above. Doing this allows the table in the Hive metastore to automatically inherit the schema, partitioning, and table properties of the existing data.\n\n**IMPORTANT**: You will need to add the name of your ADLS Gen2 account into the location value below, before running the cell."],"metadata":{}},{"cell_type":"code","source":["%sql\nCREATE TABLE transactions\nUSING DELTA\nLOCATION 'abfss://transactions@<your-adls-gen2-account-name>.dfs.core.windows.net/delta/transactions' -- TODO: Replace <your-adls-gen2-account-name> with your ADLS Gen2 account name."],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["You can use the `DESCRIBE DETAIL` SQL command to view information about schema, partitioning, table size, and so on."],"metadata":{}},{"cell_type":"code","source":["%sql\nDESCRIBE DETAIL transactions"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Finally, you can use Spark SQL to query records in the Hive table."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM transactions LIMIT 10"],"metadata":{},"outputs":[],"execution_count":15}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.6.8","nbconvert_exporter":"python","file_extension":".py"},"name":"Process-Cosmos-DB-Change-Feed","notebookId":668412331662379},"nbformat":4,"nbformat_minor":0}
